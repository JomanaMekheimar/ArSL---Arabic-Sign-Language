import os
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='librosa')

import numpy as np
import pandas as pd
import seaborn as sns
import librosa
import shutil
from io import BytesIO
from PIL import Image, ImageOps, ImageFile
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
import torch
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import torch.nn as nn
import torch.optim as optim
from torchvision import models
from tqdm import tqdm
from tensorflow.keras.preprocessing.image import load_img, img_to_array,ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping
from tensorflow.keras import mixed_precision
# Handle truncated image warnings in PIL
ImageFile.LOAD_TRUNCATED_IMAGES = True 

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

path =r"D:\7th Semester\ArtificialIntelligence\Arabic Sign Language\RGBArSLdataset"
img_size = 224
batch_size = 32
val_split = 0.2
seed = 42
from tqdm import tqdm
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
scaler = torch.amp.GradScaler(device="cuda")
classes = sorted(os.listdir(path))
num_classes = len(classes)
print("Number of classes:", num_classes)
print("Class indices mapping:", {class_name: idx for idx, class_name in enumerate(classes)})

class_counts = {}
for class_name in os.listdir(path):
    class_path = os.path.join(path, class_name)
    if os.path.isdir(class_path):
        class_counts[class_name] = len(os.listdir(class_path))

df_counts = pd.DataFrame(list(class_counts.items()), columns=["Class", "Count"])
df_counts = df_counts.sort_values("Count", ascending=False)

plt.figure(figsize=(12,6))
sns.barplot(data=df_counts, x="Class", y="Count", palette="viridis")
plt.xticks(rotation=90)
plt.title("Number of Images per Class")
plt.show()


import random
plt.figure(figsize=(12,12))
classes = os.listdir(path)
for i in range(9):
    class_choice = random.choice(classes)
    class_path = os.path.join(path, class_choice)
    img_choice = random.choice(os.listdir(class_path))
    img_path = os.path.join(class_path, img_choice)

    img = plt.imread(img_path)
    plt.subplot(3,3,i+1)
    plt.imshow(img, cmap="gray")
    plt.title(class_choice)
    plt.axis("off")
plt.show()

torch.manual_seed(seed)

# Training
train_transforms = transforms.Compose([
    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomAffine(0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225]),
])

# Validation
val_transforms = transforms.Compose([
    transforms.Resize(int(img_size * 1.15)),
    transforms.CenterCrop(img_size),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225]),
])


# Load full dataset
full_dataset = datasets.ImageFolder(root=path, transform=train_transforms)

# Split into train and val
val_size = int(len(full_dataset) * val_split)
train_size = len(full_dataset) - val_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

# Apply different transforms for validation set
val_dataset.dataset.transform = val_transforms

# Data loaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)

# Class names
class_names = full_dataset.classes

num_classes = len(class_names)

print("Classes:", class_names)
print("Number of classes:", num_classes) 

with open(r"D:\7th Semester\ArtificialIntelligence\Arabic Sign Language\class_names.txt", "w", encoding="utf-8") as f:
    for c in class_names:
        f.write(c + "\n")

import tensorflow as tf
print(tf.__version__)

base_model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)

# Freeze backbone
for param in base_model.features.parameters():
    param.requires_grad = False

# Replace classifier head 
base_model.classifier = nn.Sequential(
    nn.Dropout(0.4),                               
    nn.Linear(base_model.last_channel, 512),
    nn.ReLU(),
    nn.BatchNorm1d(512),
    nn.Dropout(0.4),
    nn.Linear(512, num_classes)                     
)

model = base_model.to(device)

criterion = nn.CrossEntropyLoss() # optimizer setup
optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)


from copy import deepcopy

epochs = 25
patience = 7
best_loss = float("inf")
early_stop_counter = 0
best_model_wts = deepcopy(model.state_dict())

# Add learning rate scheduler
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=1e-3,
    steps_per_epoch=len(train_loader),
    epochs=epochs
) 

os.makedirs(r"D:\7th Semester\ArtificialIntelligence\Arabic Sign Language\weights", exist_ok=True)
checkpoint_path = r"D:\7th Semester\ArtificialIntelligence\Arabic Sign Language\weights/best_model.pth"
final_model_path =r"D:\7th Semester\ArtificialIntelligence\Arabic Sign Language\weights/final_model.pth"

for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}/{epochs}") 
    model.train()
    running_loss, correct, total = 0.0, 0, 0

    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        with torch.amp.autocast("cuda"):   
            outputs = model(images)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()

        running_loss += loss.item() * images.size(0)
        _, preds = torch.max(outputs, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    train_acc = correct / total
    train_loss = running_loss / total   #  Validation
    model.eval()
    running_loss, correct, total = 0.0, 0, 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            with torch.amp.autocast("cuda"):  
                outputs = model(images)
                loss = criterion(outputs, labels)

            running_loss += loss.item() * images.size(0)
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    val_acc = correct / total
    val_loss = running_loss / total
    
    print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
    print(f"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}")
     # Early Stopping + Checkpoint 
    if val_loss < best_loss:
        best_loss = val_loss
        best_model_wts = deepcopy(model.state_dict())
        torch.save(best_model_wts, checkpoint_path)  
        print(f"Best model saved at epoch {epoch+1}")
        early_stop_counter = 0
    else:
        early_stop_counter += 1
        if early_stop_counter >= patience:
            print("Early stopping triggered!")
            break

#  Load Best Weights 
model.load_state_dict(torch.load(checkpoint_path))
print("Loaded best model weights.")

#  Save Final Model
torch.save(model.state_dict(), final_model_path)
print(f" Final model saved to {final_model_path}") 

num_classes = len(class_names) 


base_model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)

# Freeze 
for param in base_model.features.parameters():
    param.requires_grad = False

# Custom classifier 
base_model.classifier = nn.Sequential(
    nn.Dropout(0.4),                               
    nn.Linear(base_model.last_channel, 512),
    nn.ReLU(),
    nn.BatchNorm1d(512),
    nn.Dropout(0.4),
    nn.Linear(512, num_classes)                     
)

# Move to "cpu"
mobilenet_model = base_model.to(device)# Load saved weights
mobilenet_model.load_state_dict(torch.load(r"D:\7th Semester\ArtificialIntelligence\Arabic Sign Language\weights/final_model.pth", map_location=device))
mobilenet_model.eval()
all_preds, all_labels = [],[]

with torch.no_grad():
    for images, labels in val_loader:   
        images, labels = images.to(device), labels.to(device)
        with torch.amp.autocast("cuda"):
            outputs = mobilenet_model(images)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        
print("\nClassification Report:")
print(classification_report(all_labels, all_preds, target_names=class_names)) 
model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for images, labels in val_loader:   
        images, labels = images.to(device), labels.to(device)
        with torch.amp.autocast("cuda"):
            outputs = model(images)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Classification Report ''
print("\n Classification Report:")
print(classification_report(all_labels, all_preds, target_names=class_names))

# Confusion Matrix
cm = confusion_matrix(all_labels, all_preds)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(30, 15))
sns.heatmap(cm_normalized, annot=True, fmt=".2f", cmap="Blues",
            xticklabels=class_names,
            yticklabels=class_names)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for images, labels in val_loader:   
        images, labels = images.to(device), labels.to(device)
        with torch.amp.autocast("cuda"):
            outputs = model(images)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Classification Report ''
print("\n Classification Report:")
print(classification_report(all_labels, all_preds, target_names=class_names))

# Confusion Matrix 
cm = confusion_matrix(all_labels, all_preds)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(30,15))
sns.heatmap(cm_normalized, annot=True, fmt=".2f", cmap="Blues",
            xticklabels=class_names,
            yticklabels=class_names)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

